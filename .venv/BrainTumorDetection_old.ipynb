{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importieren der notwendigen Bibliotheken\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dataset-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset-Klasse für Gehirntumor-Bilder\n",
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Initialisiert den Dataset für Gehirntumor-Klassifikation.\n",
    "        Args:\n",
    "        - image_dir: Pfad zum Ordner 'data', der die Unterordner 'yes/' und 'no/' enthält.\n",
    "        - transform: Transformationen für die Vorverarbeitung.\n",
    "        \"\"\"\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        # Pfade zu den Unterordnern \"yes\" (Tumor) und \"no\" (Healthy)\n",
    "        tumor_dir = Path('data/yes/*.jpg') / \"yes\"\n",
    "        healthy_dir = Path('data/no/*.jpg') / \"no\"\n",
    "\n",
    "        # Bilder aus \"yes\" (Tumor) mit Label 1\n",
    "        self.image_paths.extend(tumor_dir.rglob(\"*.jpg\"))\n",
    "        self.labels.extend([1] * len(list(tumor_dir.rglob(\"*.jpg\"))))\n",
    "\n",
    "        # Bilder aus \"no\" (Healthy) mit Label 0\n",
    "        self.image_paths.extend(healthy_dir.rglob(\"*.jpg\"))\n",
    "        self.labels.extend([0] * len(list(healthy_dir.rglob(\"*.jpg\"))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Transformationen für die Datenvorbereitung\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "model-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vulpe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vulpe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transfer Learning Modell mit ResNet-18\n",
    "class TransferLearningModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransferLearningModel, self).__init__()\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Initialisierung von Modell, Loss-Funktion und Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransferLearningModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.base_model.fc.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "training-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Trainings- und Validierungsprozess\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Trainiert das Modell und validiert es in jedem Epoch.\n",
    "    Args:\n",
    "    - model: Das zu trainierende Modell.\n",
    "    - dataloaders: Dictionary mit 'train' und 'val' Dataladern.\n",
    "    - criterion: Loss-Funktion.\n",
    "    - optimizer: Optimizer für das Training.\n",
    "    - num_epochs: Anzahl der Epochen.\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "data-prep-training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamtanzahl der Bilder im Dataset: 800\n",
      "Anzahl der Tumor-Bilder: 392\n",
      "Anzahl der gesunden Bilder: 408\n",
      "Anzahl der Trainings-Samples: 640\n",
      "Anzahl der Validierungs-Samples: 160\n",
      "Epoch 1/10\n",
      "----------\n",
      "train Loss: 0.5786 Acc: 0.7125\n",
      "val Loss: 0.3818 Acc: 0.8250\n",
      "Epoch 2/10\n",
      "----------\n",
      "train Loss: 0.4044 Acc: 0.8156\n",
      "val Loss: 0.2271 Acc: 0.9250\n",
      "Epoch 3/10\n",
      "----------\n",
      "train Loss: 0.2883 Acc: 0.8812\n",
      "val Loss: 0.2031 Acc: 0.9187\n",
      "Epoch 4/10\n",
      "----------\n",
      "train Loss: 0.2285 Acc: 0.9141\n",
      "val Loss: 0.2246 Acc: 0.9000\n",
      "Epoch 5/10\n",
      "----------\n",
      "train Loss: 0.3110 Acc: 0.8594\n",
      "val Loss: 0.1956 Acc: 0.9250\n",
      "Epoch 6/10\n",
      "----------\n",
      "train Loss: 0.2256 Acc: 0.8969\n",
      "val Loss: 0.2017 Acc: 0.9000\n",
      "Epoch 7/10\n",
      "----------\n",
      "train Loss: 0.2161 Acc: 0.9125\n",
      "val Loss: 0.1848 Acc: 0.9125\n",
      "Epoch 8/10\n",
      "----------\n",
      "train Loss: 0.2017 Acc: 0.9156\n",
      "val Loss: 0.1736 Acc: 0.9375\n",
      "Epoch 9/10\n",
      "----------\n",
      "train Loss: 0.1988 Acc: 0.9250\n",
      "val Loss: 0.1760 Acc: 0.9187\n",
      "Epoch 10/10\n",
      "----------\n",
      "train Loss: 0.2006 Acc: 0.9078\n",
      "val Loss: 0.1698 Acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "# Dataset-Klasse für Gehirntumor-Bilder\n",
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        \"\"\"\n",
    "        Initialisiert den Dataset für Gehirntumor-Klassifikation.\n",
    "        Args:\n",
    "        - transform: Transformationen für die Vorverarbeitung.\n",
    "        \"\"\"\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        # Direktes Laden der Bildpfade\n",
    "        tumor_dir = Path('data/yes')  # Pfad zu Tumor-Bildern\n",
    "        healthy_dir = Path('data/no')  # Pfad zu gesunden Bildern\n",
    "\n",
    "        # Bilder aus \"yes\" (Tumor) mit Label 1\n",
    "        self.image_paths.extend(list(tumor_dir.rglob(\"*.jpg\")))\n",
    "        self.labels.extend([1] * len(list(tumor_dir.rglob(\"*.jpg\"))))\n",
    "\n",
    "        # Bilder aus \"no\" (Healthy) mit Label 0\n",
    "        self.image_paths.extend(list(healthy_dir.rglob(\"*.jpg\")))\n",
    "        self.labels.extend([0] * len(list(healthy_dir.rglob(\"*.jpg\"))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Transformationen für die Datenvorbereitung\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Laden des Datasets\n",
    "dataset = BrainTumorDataset(transform=data_transforms)\n",
    "\n",
    "# Debugging: Gesamtanzahl der Bilder überprüfen\n",
    "print(f\"Gesamtanzahl der Bilder im Dataset: {len(dataset)}\")\n",
    "print(f\"Anzahl der Tumor-Bilder: {sum(label == 1 for label in dataset.labels)}\")\n",
    "print(f\"Anzahl der gesunden Bilder: {sum(label == 0 for label in dataset.labels)}\")\n",
    "\n",
    "# Prüfen, ob genügend Bilder für das Training und die Validierung vorhanden sind\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "if train_size == 0 or val_size == 0:\n",
    "    raise ValueError(\"Trainings- oder Validierungsset hat keine ausreichenden Samples!\")\n",
    "\n",
    "# Aufteilen des Datasets in Trainings- und Validierungsdaten\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader für Training und Validierung erstellen\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=16, shuffle=True),\n",
    "    'val': DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "}\n",
    "\n",
    "# Debugging: Überprüfen der DataLoader-Größen\n",
    "print(f\"Anzahl der Trainings-Samples: {len(train_dataset)}\")\n",
    "print(f\"Anzahl der Validierungs-Samples: {len(val_dataset)}\")\n",
    "\n",
    "# Training starten\n",
    "model = train_model(model, dataloaders, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "save-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell gespeichert.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Speichern des Modells\n",
    "torch.save(model.state_dict(), \"brain_tumor_model.pth\")\n",
    "print(\"Modell gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c103b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vulpe\\AppData\\Local\\Temp\\ipykernel_47308\\4132344695.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"brain_tumor_model.pth\"))  # Pfad zum gespeicherten Modell\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "# Transformationen wie während des Trainings\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Modell laden\n",
    "model = TransferLearningModel()  # Erstellen Sie die Modellinstanz\n",
    "model.load_state_dict(torch.load(\"brain_tumor_model.pth\"))  # Pfad zum gespeicherten Modell\n",
    "model.eval()  # Schalten Sie das Modell in den Evaluierungsmodus\n",
    "\n",
    "# Gerät festlegen (GPU oder CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Pfad zum Testbild\n",
    "test_image_path = Path(\"data/new/image.jpg\")  # Pfad zum Testbild\n",
    "\n",
    "# Bild laden und vorbereiten\n",
    "image = Image.open(test_image_path).convert(\"RGB\")  # Bild öffnen und in RGB konvertieren\n",
    "input_tensor = data_transforms(image).unsqueeze(0).to(device)  # Transformation und Batch-Dimension hinzufügen\n",
    "\n",
    "# Vorhersage\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)  # Modellvorhersage\n",
    "    _, predicted_class = torch.max(output, 1)  # Klasse mit der höchsten Wahrscheinlichkeit\n",
    "\n",
    "# Ausgabe der Vorhersage\n",
    "if predicted_class.item() == 1:\n",
    "    print(\"Tumor\")\n",
    "else:\n",
    "    print(\"Healthy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
